The U-Net segmentation model has been enhanced with the incorporation of a Monte Carlo (MC) Dropout layer. The term "dropout" refers to dropping out units (hidden and visible) in a neural network. By default, the dropout probability is set to 0.5. For more information, refer to N. Srivastava et al.'s paper [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/v15/srivastava14a.html) published in The journal of machine learning research 15 (1), 1929-1958. However, one can adjust this value to observe variations in predictions and assess the model's sensitivity. The uncertainty estimation will be updated with using multiple forward passes. For now, this model is a very simplistic version to Quantify the Uncertainty. This adaptation of dropout during training is a somewhat dirty implementation of Bayesian Neural Networks. Some images and basic architecture are adapted from [Kaggle](https://www.kaggle.com/code/s0mnaths/brain-mri-unet-pytorch/notebook).
 
